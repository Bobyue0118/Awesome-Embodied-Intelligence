# Awesome Embodied Intelligence: A Synergy of Morphology, Action, Perception and Learning

This repo contains all the citations in Section 3 *Architecture of Embodied Intelligence* of [Embodied Intelligence: A Synergy of Morphology, Action, Perception and Learning](https://dl.acm.org/doi/pdf/10.1145/3717059).


### 3.1 Action Generation Based on Learning (L → A)

- [9] Richard Bellman. 1957. A Markovian decision process. *Journal of Mathematics and Mechanics*, 6(5), 679–684.
- [196] Christopher J. C. H. Watkins. 1989. Learning from Delayed Rewards. Ph.D. dissertation, Royal Holloway, University of London.
- [127] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement learning. *Nature*, 518(7540), 529–533.
- [134] Emre O. Neftci and Bruno B. Averbeck. 2019. Reinforcement learning in artificial and biological systems. *Nature Machine Intelligence*, 1(3), 133–143.
- [187] George A. Vouros. 2022. Explainable deep reinforcement learning: State of the art and challenges. *ACM Computing Surveys*, 55(5), 1–39.
- [167] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.
- [66] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In *Proceedings of the International Conference on Machine Learning*, 1861–1870.

### 3.2 Action Generation Based on Morphology (M → A)

- [72] Van Anh Ho, Hongbin Liu, Liyu Wang, Fumiya Iida, and Shinichi Hirai. 2018. Special issue on ‘Morphological computation in soft robotics.’ *Advanced Robotics*, 32(7), 339–339.
- [138] Kohei Nakajima, Helmut Hauser, Tao Li, and Rolf Pfeifer. 2015. Information processing via physical soft body. *Scientific Reports*, 5(10487).
- [23] Rolf Pfeifer, Max Lungarella, and Fumiya Iida. 2007. Self-organization, embodiment, and biologically inspired robotics. *Science*, 318(5853), 1088–1093.
- [37] Steve Collins, Andy Ruina, Russ Tedrake, and Martijn Wisse. 2005. Efficient bipedal robots based on passive-dynamic walkers. Science 307, 5712 (2005), 1082–1085.
- [125] Tad McGeer. 1990. Passive dynamic walking. *International Journal of Robotics Research*, 9(2), 62–82.
- [52] Julius E. Bernth, Van Anh Ho, and Hongbin Liu. 2018. Morphological computation in haptic sensation and interaction: From nature to robotics. *Advanced Robotics*, 32(7), 340–362.
- [103] Longchuan Li, Shugen Ma, Isao Tokuda, Fumihiko Asano, Makoto Nokata, Yang Tian, and Liang Du. 2021. Generation of efficient rectilinear gait based on dynamic morphological computation and its theoretical analysis. *IEEE Robotics and Automation Letters*, 6(2), 841–848.
- [10] Julius E. Bernth, Van Anh Ho, and Hongbin Liu. 2018. Morphological computation in haptic sensation and interaction: From nature to robotics. *Advanced Robotics*, 32(7), 340–362.
- [201] Matthew A. Woodward and Metin Sitti. 2018. Morphological intelligence counters foot slipping in the desert locust and dynamic robots. *PNAS*, 115(36), E8358–E8367.
- [95] Sylvain Koos, Jean‐Baptiste Mouret, and Stéphane Doncieux. 2012. The transferability approach: Crossing the reality gap in evolutionary robotics. *IEEE Transactions on Evolutionary Computation*, 17(1), 122–145.
- [59] Keyan Ghazi-Zahedi, Daniel F. B. Haeufle, Guido Montúfar, Syn Schmitt, and Nihat Ay. 2016. Evaluating morphological computation in muscle and DC-motor driven models of hopping movements. *Frontiers in Robotics and AI*, 3(42).
- [92] Peter Battaglia, et al. 2018. Graph networks as learnable physics engines for inference and control. In *ICML*, 4470–4479.

### 3.3 Morphological Control Based on Action (A → M)

- [31] Antonio Bicchi and Vijay Kumar. 2000. Robotic grasping and contact: A review. In *ICRA’00*, Vol. 1, 348–353.
- [192] Tingwu Wang, Renjie Liao, Jimmy Ba, and Sanja Fidler. 2018. NerveNet: Learning structured policy with graph neural networks. In *ICLR*.
- [185] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gómez, L. Kaiser, and I. Polosukhin. 2017. Attention is all you need. In *NeurIPS*, 6000–6010.
- [69] Donald Hejna, Lerrel Pinto, and Pieter Abbeel. 2020. Hierarchically decoupled imitation for morphological transfer. In *ICML*, 4159–4171.

### 3.4 Perception-Driven Morphology Transformation (P → M)

- [171] Vitaly Kurin, Maximilian Igl, Tim Rocktäschel, Wendelin Boehmer, and Shimon Whiteson. 2020. My body is a cage: The role of morphology in graph-based incompatible control. arXiv preprint arXiv:2010.01856.
- [140] Tønnes F. Nygaard, Charles P. Martin, Jim Torresen, and Kyrre Glette. 2019. Self-modifying morphology experiments with DyRET: Dynamic robot for embodied testing. In *ICRA’19*, 9446–9452.
- [177] Ana Pervan and Todd D. Murphey. 2020. Algorithmic design for embodied intelligence in synthetic cells. *IEEE Transactions on Automation Science and Engineering*, 18(3), 864–875.
- [94] Sam Kriegman, Amir Mohammadi Nasab, Dylan Shah, Hannah Steele, Gabrielle Branin, Michael Levin, Josh Bongard, and Rebecca Kramer-Bottiglio. 2020. Scalable sim-to-real transfer of soft robot designs. In *RoboSoft’20*, 359–366.
- [202] Jian Wu, Shi-Yang Tang, Tao Fang, Weihua Li, Xiangpeng Li, and Shiwu Zhang. 2018. A wheeled robot driven by a liquid-metal droplet. *Advanced Materials*, 30(51), 1805039.

### 3.5 Learning-Driven Morphology Optimization (L → M)

- [135] Andrew L. Nelson, Gregory J. Barlow, and Lefteris Doitsidis. 2009. Fitness functions in evolutionary robotics: A survey and analysis. *Robotics and Autonomous Systems*, 57(4), 345–352.
- [151] Rolf Pfeifer and Josh Bongard. 2006. How the Body Shapes the Way We Think: A New View of Intelligence. MIT Press.
- [172] Paolo Pagliuca and Stefano Nolfi. 2022. The dynamic of body and brain co-evolution. *Adaptive Behavior*, 30(3), 245–255.
- [4] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. 2019. Habitat: A platform for embodied AI research. In *ICCV*, 9339–9347.
- [19] Sebastian Thrun and Tom M. Mitchell. 1995. Lifelong robot learning. *Robotics and Autonomous Systems*, 15(1-2), 25–46.
- [76] Gregory S. Hornby and Jordan B. Pollack. 2001. Body-brain co-evolution using L-systems as a generative encoding. In *GECCO*, 868–875.
- [107] Hod Lipson and Jordan B. Pollack. 2000. Automatic design and manufacture of robotic lifeforms. *Nature*, 406(6799), 974–978.
- [119] Jiaping Lin, et al. 2024. *Beyond structures: Insights into morphology-control synergy*. arXiv preprint arXiv:2401.00000. *(placeholder)*
- [124] Craig Mautner and Richard K. Belew. 2000. Evolving robot morphology and control. *Artificial Life and Robotics*, 4, 130–136.
- [174] Charles Schaff, David Yunis, Ayan Chakrabarti, and Matthew R. Walter. 2019. Jointly learning to construct and control agents using deep reinforcement learning. In *ICRA’19*, 9798–9805.

### 3.6 Perception-Driven Action Generation (P → A)

- [13] Antonio Bicchi and Vijay Kumar. 2000. Robotic grasping and contact: A review. In *ICRA’00*, Vol. 1, 348–353.
- [136] Rhys Newbury, Morris Gu, Lachlan Chumbley, Arsalan Mousavian, Clemens Eppner, Jürgen Leitner, Jeannette Bohg, Antonio Morales, Tamim Asfour, Danica Kragic, et al. 2023. Deep learning approaches to grasp synthesis: A review. *IEEE Transactions on Robotics*, 39(5), 3994–4015.
- [22] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, et al. 2019. Habitat: A platform for embodied AI research. *ICCV*, 9339–9347.
- [211] Pierre Marza, Laetitia Matignon, Olivier Simonin, Dhruv Batra, Christian Wolf, and Devendra Singh Chaplot. 2023. AutoNeRF: Training implicit scene representations with autonomous agents. arXiv preprint arXiv:2304.11241.
- [226] Yiming Zuo, Weichao Qiu, Lingxi Xie, Fangwei Zhong, Yizhou Wang, and Alan L. Yuille. 2019. Craves: Controlling robotic arm with a vision-based economic system. In *CVPR*, 4214–4223.
- [166] Lukas Schmid, Marcus Abate, Yun Chang, and Luca Carlone. 2024. Khronos: A unified approach for spatio-temporal metric-semantic SLAM in dynamic environments. In *RSS 2024*.
- [195] Yanan Wang, Yaobin Tian, Jiawei Chen, Kun Xu, and Xilun Ding. 2024. A survey of visual SLAM in dynamic environment: The evolution from geometric to semantic approaches. *IEEE Transactions on Instrumentation and Measurement*, 73, 2523221.
- [204] Wansen Wu, Tao Chang, Xinmeng Li, Quanjun Yin, and Yue Hu. 2024. Vision-language navigation: A survey and taxonomy. *Neural Computing and Applications*, 36(7), 3291–3316.
- [205] Xuesu Xiao, Bo Liu, Garrett Warnell, and Peter Stone. 2022. Motion planning and control for mobile robot navigation using machine learning: A survey. *Autonomous Robots*, 46(5), 569–597.

### 3.7 Action-Driven Perception Improvement (A → P)

- [7] Ruzena Bajcsy. 1988. Active perception. *Proceedings of the IEEE*, 76(8), 966–1005.
- [2] John Aloimonos, Isaac Weiss, and Amit Bandyopadhyay. 1988. Active vision. *International Journal of Computer Vision*, 1, 333–356.
- [45] Timothy Patten, Michael Zillich, Robert Fitch, Markus Vincze, and Salah Sukkarieh. 2015. Viewpoint evaluation for online 3-D active object classification. *IEEE Robotics and Automation Letters*, 1(1), 73–81.
- [147] Timothy Patten, Michael Zillich, Robert Fitch, Markus Vincze, and Salah Sukkarieh. 2015. Viewpoint evaluation for online 3-D active object classification. *IEEE Robotics and Automation Letters*, 1(1), 73–81.
- [143] Lucas Paletta and Axel Pinz. 2000. Active object recognition by view integration and reinforcement learning. *Robotics and Autonomous Systems*, 31(1-2), 71–86.
- [67] Xiaoning Han, Huaping Liu, Fuchun Sun, and Xinyu Zhang. 2019. Active object detection with multistep action prediction using deep Q-network. *IEEE Transactions on Industrial Informatics*, 15(6), 3723–3731.
- [84] Dinesh Jayaraman and Kristen Grauman. 2018. End-to-end policy learning for active visual categorization. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 41(7), 1601–1614.
- [222] Fangwei Zhong, Peng Sun, Wenhan Luo, Tingyun Yan, and Yizhou Wang. 2019. AD-VAT+: An asymmetric dueling mechanism for learning and understanding visual active tracking. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 43(5), 1467–1482.
- [186] Tiago Veiga and Jennifer Renoux. 2023. From reactive to active sensing: A survey on information gathering in decision-theoretic planning. *ACM Computing Surveys*, 55(13s), Article 280.

### 3.8 Action-Driven Embodied Learning (A → L)

- [28] Abigail Xinghang Li, Di Guo, Huaping Liu, and Fuchun Sun. 2022. Embodied semantic scene graph generation. In *CoRL*, 1585–1594.
- [26] Xinghang Li, Di Guo, Huaping Liu, and Fuchun Sun. 2022. Embodied semantic scene graph generation. In *CoRL*, 1585–1594.
- [190] Juan Wang and Huaping Liu. 2023. Self-supervised embodied learning for semantic segmentation. In *ICDL’23*, 383–390.
- [87] Ya Jing and Tao Kong. 2023. Learning to explore informative trajectories and samples for embodied perception. arXiv preprint arXiv:2303.10936.
- [219] Qianfan Zhao, Lu Zhang, Lingxi Wu, Hong Qiao, and Zhiyong Liu. 2022. A real 3D embodied dataset for robotic active visual learning. *IEEE Robotics and Automation Letters*, 7(3), 6646–6652.
- [198] Luca Weihs, Aniruddha Kembhavi, Kiana Ehsani, Sarah M. Pratt, Winson Han, Alvaro Herrasti, Eric Kolve, Dustin Schwenk, Roozbeh Mottaghi, and Ali Farhadi. 2021. Learning generalizable visual representations via interactive gameplay. In *ICLR’21*.
- [21] Julian Whitman, Matthew Travers, and Howie Choset. 2023. Learning modular robot control policies. *IEEE Transactions on Robotics*, 39(5), 4095–4113.
- [154] Lerrel Pinto, Dhiraj Gandhi, Yuanfeng Han, Yong-Lae Park, and Abhinav Gupta. 2016. The curious robot: Learning visual representations via physical interactions. In *ECCV*, 3–18.
- [207] Zhenjia Xu, Jiajun Wu, Andy Zeng, Joshua B. Tenenbaum, and Shuran Song. 2019. DensePhysNet: Learning dense physical object representations via multi-step dynamic interactions. arXiv preprint arXiv:1906.03853.
